{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import mmap\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "log_file = \"notifications.log\"\n",
    "\n",
    "def send_notification(title, message):\n",
    "    subprocess.run([\"notify-send\", title, message])\n",
    "    with open(log_file, \"a\") as f:\n",
    "        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        f.write(f\"[{timestamp}] {title}: {message}\\n\")\n",
    "\n",
    "send_notification(\"Sanskrit Training\",\"Sanskrit Training Started\")\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "send_notification(\"Sanskrit Training\",\"Sanskrit model training started...\")\n",
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "block_size = 128\n",
    "max_iters = 1500\n",
    "learning_rate = 3e-4\n",
    "eval_iters = 100\n",
    "n_embd = 384\n",
    "n_head = 8\n",
    "n_layer = 8\n",
    "dropout = 0.2\n",
    "\n",
    "send_notification(\"Sanskrit Training\", device)\n",
    "\n",
    "with open('/home/anon/dataset/Sankrit_Corpus/vocab.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "chars = sorted(set(text))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "str_to_int = {ch: i for i, ch in enumerate(chars)}\n",
    "int_to_str = {i: ch for i, ch in enumerate(chars)}\n",
    "encode = lambda s: [str_to_int[c] for c in s]\n",
    "decode = lambda l: [int_to_str[i] for i in l]\n",
    "\n",
    "# Memory map for using small snippets of text from a single file of any size\n",
    "def get_random_chunk(split):\n",
    "    filename = \"/home/anon/dataset/Sankrit_Corpus/train_split.txt\" if split == 'train' else \"/home/anon/dataset/Sankrit_Corpus/val_split.txt\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        with mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ) as mm:\n",
    "            file_size = len(mm)\n",
    "            start_pos = random.randint(0, file_size - block_size * batch_size)\n",
    "            mm.seek(start_pos)\n",
    "            block = mm.read(block_size * batch_size - 1)\n",
    "            decoded_block = block.decode('utf-8', errors='ignore').replace('\\r', '')\n",
    "            data = torch.tensor(encode(decoded_block), dtype=torch.long)\n",
    "    return data\n",
    "\n",
    "def get_batch(split, batch_size):\n",
    "    data = get_random_chunk(split)\n",
    "    data_len = len(data)\n",
    "    if data_len <= block_size + 1:\n",
    "        return None, None  # Not enough data for a batch\n",
    "    batch_size = min(batch_size, (data_len - 1) // block_size)\n",
    "    ix = torch.randint(data_len - block_size - 1, (batch_size,))\n",
    "    x = torch.stack([data[i:i + block_size] for i in ix])\n",
    "    y = torch.stack([data[i + 1:i + block_size + 1] for i in ix])\n",
    "    valid_mask = (y < vocab_size).all(dim=1)\n",
    "    x = x[valid_mask]\n",
    "    y = y[valid_mask]\n",
    "    if len(x) == 0 or len(y) == 0:\n",
    "        return None, None  # No valid batches\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "def bleu_score(reference, candidate, k=4):\n",
    "    def n_gram_precision(ref, cand, n):\n",
    "        ref_ngrams = Counter([tuple(ref[i:i+n]) for i in range(len(ref)-n+1)])\n",
    "        cand_ngrams = Counter([tuple(cand[i:i+n]) for i in range(len(cand)-n+1)])\n",
    "        numerator = sum((cand_ngrams & ref_ngrams).values())\n",
    "        denominator = max(1, sum(cand_ngrams.values()))\n",
    "        return numerator / denominator\n",
    "\n",
    "    precisions = [n_gram_precision(reference, candidate, i) for i in range(1, k+1)]\n",
    "    bleu = np.exp(sum(np.log(p) if p > 0 else 0 for p in precisions) / k)\n",
    "    bp = np.exp(1 - len(reference) / len(candidate)) if len(candidate) < len(reference) else 1.0\n",
    "    return bleu * bp\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss_and_metrics(model):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        bleu_scores = []\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split, batch_size)\n",
    "            if X is None:\n",
    "                continue  # Skip this iteration if not enough data for a batch\n",
    "            X = X.view(-1, X.size(-1))\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "            preds = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "            targets = Y.cpu().numpy()\n",
    "            preds_flat = preds.flatten()\n",
    "            targets_flat = targets.flatten()\n",
    "            all_preds.extend(preds_flat)\n",
    "            all_targets.extend(targets_flat)\n",
    "            preds_list = [np.atleast_1d(p).tolist() for p in preds_flat]\n",
    "            targets_list = [np.atleast_1d(t).tolist() for t in targets_flat]\n",
    "            for p, t in zip(preds_list, targets_list):\n",
    "                p_sentence = decode(p)\n",
    "                t_sentence = decode(t)\n",
    "                bleu_scores.append(bleu_score(t_sentence, p_sentence))\n",
    "        cm = confusion_matrix(all_targets, all_preds)\n",
    "        tp = np.diag(cm).sum()\n",
    "        fp = cm.sum(axis=0) - np.diag(cm)\n",
    "        fn = cm.sum(axis=1) - np.diag(cm)\n",
    "        tn = cm.sum() - (fp + fn + tp)\n",
    "        out[split] = {\n",
    "            'loss': losses.mean().item(),\n",
    "            'accuracy': accuracy_score(all_targets, all_preds),\n",
    "            'precision': precision_score(all_targets, all_preds, average='macro', zero_division=0),\n",
    "            'recall': recall_score(all_targets, all_preds, average='macro', zero_division=0),\n",
    "            'f1': f1_score(all_targets, all_preds, average='macro', zero_division=0),\n",
    "            'bleu': sum(bleu_scores) / len(bleu_scores) if bleu_scores else 0.0,\n",
    "            'true_positive': tp,\n",
    "            'true_negative': tn.sum(),\n",
    "            'false_positive': fp.sum(),\n",
    "            'false_negative': fn.sum()\n",
    "        }\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        wei = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class GPTLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=idx.device))\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_new_tokens):\n",
    "                idx_cond = idx[:, -block_size:]\n",
    "                logits, _ = self(idx_cond)\n",
    "                logits = logits[:, -1, :]\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                idx_next = torch.multinomial(probs, num_samples=1)\n",
    "                idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "\n",
    "model = GPTLanguageModel().to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "if os.path.exists('/home/anon/training/model_sans.pkl'):\n",
    "    send_notification(\"Sanskrit Training\",'loading model parameters...')\n",
    "    with open('/home/anon/training/model_sans.pkl', 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    send_notification(\"Sanskrit Training\",'loaded successfully!')\n",
    "m = model.to(device)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    xb, yb = get_batch('train', batch_size)\n",
    "    if xb is None:\n",
    "        continue  # Skip this iteration if not enough data for a batch\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if iter % eval_iters == 0:\n",
    "        losses = estimate_loss_and_metrics(model)\n",
    "        end_time = time.time()\n",
    "        send_notification(\"Sanskrit Training\", f\"step: {iter}, train loss: {losses['train']['loss']:.3f}, val loss: {losses['val']['loss']:.3f}, \"+\n",
    "                                                f\"train accuracy: {losses['train']['accuracy']:.3f}, val accuracy: {losses['val']['accuracy']:.3f}, \"+\n",
    "                                                f\"train precision: {losses['train']['precision']:.3f}, val precision: {losses['val']['precision']:.3f}, \"+\n",
    "                                                f\"train recall: {losses['train']['recall']:.3f}, val recall: {losses['val']['recall']:.3f}, \"+\n",
    "                                                f\"train f1: {losses['train']['f1']:.3f}, val f1: {losses['val']['f1']:.3f}, \"+\n",
    "                                                f\"train BLEU: {losses['train']['bleu']:.3f}, val BLEU: {losses['val']['bleu']:.3f}, \"+\n",
    "                                                f\"train TP: {losses['train']['true_positive']}, val TP: {losses['val']['true_positive']}, \"+\n",
    "                                                f\"train TN: {losses['train']['true_negative']}, val TN: {losses['val']['true_negative']}, \"+\n",
    "                                                f\"train FP: {losses['train']['false_positive']}, val FP: {losses['val']['false_positive']}, \"+\n",
    "                                                f\"train FN: {losses['train']['false_negative']}, val FN: {losses['val']['false_negative']} \"+\n",
    "                                                f\"in {(end_time - start_time):.2f} seconds.\")\n",
    "\n",
    "\n",
    "print(\"Training complete.\")\n",
    "with open('/home/anon/training/model_sans.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "send_notification(\"Sanskrit Training\",'Sanskrit model saved')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
