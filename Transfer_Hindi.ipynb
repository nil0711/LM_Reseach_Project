{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import mmap\n",
    "import random\n",
    "import pickle\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "log_file = \"notifications.log\"\n",
    "\n",
    "def send_notification(title, message):\n",
    "    subprocess.run([\"notify-send\", title, message])\n",
    "    with open(log_file, \"a\") as f:\n",
    "        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        f.write(f\"[{timestamp}] {title}: {message}\\n\")\n",
    "\n",
    "send_notification(\"Hindi Transfer Training\", \"Hindi Transfer Training Started\")\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "send_notification(\"Hindi Transfer Training\", f\"Using device: {device}\")\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "block_size = 128\n",
    "max_iters = 1400\n",
    "learning_rate = 3e-4\n",
    "eval_iters = 100\n",
    "n_embd = 384\n",
    "n_head = 8\n",
    "n_layer = 8\n",
    "dropout = 0.2\n",
    "forget_retension_percentage = 0.05\n",
    "\n",
    "# Load the vocabularies\n",
    "if os.path.exists('/home/anon/training/trans_vocab.txt'):\n",
    "    with open('/home/anon/training/trans_vocab.txt', 'r', encoding='utf-8') as f:\n",
    "        trans_text = f.read()\n",
    "    trans_vocab = set(trans_text)\n",
    "else:\n",
    "    with open('/home/anon/dataset/Sankrit_Corpus/vocab.txt', 'r', encoding='utf-8') as f:\n",
    "        sanskrit_text = f.read()\n",
    "    trans_vocab = set(sanskrit_text)\n",
    "vocab_size = len(trans_vocab)\n",
    "# Load Hindi Corpus vocabulary\n",
    "with open('/home/anon/dataset/Hindi_Corpus/vocab.txt', 'r', encoding='utf-8') as f:\n",
    "    hindi_text = f.read()\n",
    "hindi_vocab = set(hindi_text)\n",
    "\n",
    "# Encode and Decode functions\n",
    "str_to_int = {ch: i for i, ch in enumerate(trans_vocab)}\n",
    "int_to_str = {i: ch for i, ch in enumerate(trans_vocab)}\n",
    "\n",
    "def encode(s):\n",
    "    return [str_to_int.get(c, 0) for c in s]  # Map unknown characters to token 0 (or another value)\n",
    "\n",
    "decode = lambda l: [int_to_str[i] for i in l if i in int_to_str]\n",
    "\n",
    "def get_random_chunk(split):\n",
    "    filename = \"/home/anon/dataset/Hindi_Corpus/train_split.txt\" if split == 'train' else \"/home/anon/dataset/Hindi_Corpus/val_split.txt\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        with mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ) as mm:\n",
    "            file_size = len(mm)\n",
    "            start_pos = random.randint(0, file_size - block_size * batch_size)\n",
    "            mm.seek(start_pos)\n",
    "            block = mm.read(block_size * batch_size - 1)\n",
    "            decoded_block = block.decode('utf-8', errors='ignore').replace('\\r', '')\n",
    "            data = torch.tensor(encode(decoded_block), dtype=torch.long)\n",
    "    return data, decoded_block\n",
    "\n",
    "def get_batch(split, batch_size):\n",
    "    data, decoded_block = get_random_chunk(split)\n",
    "    data_len = len(data)\n",
    "    if data_len <= block_size + 1:  # Ensure at least one token for x and one for y\n",
    "        return None, None, decoded_block  # Not enough data for a batch\n",
    "    batch_size = min(batch_size, (data_len - 1) // block_size)\n",
    "    ix = torch.randint(data_len - block_size - 1, (batch_size,))\n",
    "    x = torch.stack([data[i:i + block_size] for i in ix])\n",
    "    y = torch.stack([data[i + 1:i + block_size + 1] for i in ix])\n",
    "    \n",
    "    # Filter out batches where any target index exceeds the vocabulary size\n",
    "    valid_mask = (y < vocab_size).all(dim=1)\n",
    "    x = x[valid_mask]\n",
    "    y = y[valid_mask]\n",
    "    \n",
    "    if len(x) == 0 or len(y) == 0:\n",
    "        return None, None, decoded_block  # No valid batches\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y, decoded_block\n",
    "\n",
    "def bleu_score(reference, candidate, k=4):\n",
    "    if len(candidate) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    def n_gram_precision(ref, cand, n):\n",
    "        ref_ngrams = Counter([tuple(ref[i:i+n]) for i in range(len(ref)-n+1)])\n",
    "        cand_ngrams = Counter([tuple(cand[i:i+n]) for i in range(len(cand)-n+1)])\n",
    "        \n",
    "        numerator = sum((cand_ngrams & ref_ngrams).values())\n",
    "        denominator = max(1, sum(cand_ngrams.values()))\n",
    "        return numerator / denominator\n",
    "\n",
    "    precisions = [n_gram_precision(reference, candidate, i) for i in range(1, k+1)]\n",
    "    bleu = np.exp(sum(np.log(p) if p > 0 else 0 for p in precisions) / k)\n",
    "    \n",
    "    bp = np.exp(1 - len(reference) / len(candidate)) if len(candidate) < len(reference) else 1.0\n",
    "    return bleu * bp\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss_and_metrics(model):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        bleu_scores = []\n",
    "        for k in range(eval_iters):\n",
    "            X, Y, _ = get_batch(split, batch_size)\n",
    "            if X is None:\n",
    "                continue  # Skip this iteration if not enough data for a batch\n",
    "            X = X.view(-1, X.size(-1))\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "            \n",
    "            preds = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "            targets = Y.cpu().numpy()\n",
    "            \n",
    "            preds_flat = preds.flatten()\n",
    "            targets_flat = targets.flatten()\n",
    "            \n",
    "            all_preds.extend(preds_flat)\n",
    "            all_targets.extend(targets_flat)\n",
    "            \n",
    "            preds_list = [np.atleast_1d(p).tolist() for p in preds_flat]\n",
    "            targets_list = [np.atleast_1d(t).tolist() for t in targets_flat]\n",
    "\n",
    "            for p, t in zip(preds_list, targets_list):\n",
    "                p_sentence = decode(p)\n",
    "                t_sentence = decode(t)\n",
    "                if p_sentence and t_sentence:  # Ensure both are non-empty\n",
    "                    bleu_scores.append(bleu_score(t_sentence, p_sentence))\n",
    "                \n",
    "        cm = confusion_matrix(all_targets, all_preds)\n",
    "        \n",
    "        tp = np.diag(cm).sum()\n",
    "        # Calculate false positives, false negatives, and true negatives\n",
    "        fp = cm.sum(axis=0) - np.diag(cm)\n",
    "        fn = cm.sum(axis=1) - np.diag(cm)\n",
    "        tn = cm.sum() - (fp + fn + tp)\n",
    "\n",
    "\n",
    "\n",
    "        out[split] = {\n",
    "            'loss': losses.mean().item(),\n",
    "            'accuracy': accuracy_score(all_targets, all_preds),\n",
    "            'precision': precision_score(all_targets, all_preds, average='macro', zero_division=0),\n",
    "            'recall': recall_score(all_targets, all_preds, average='macro', zero_division=0),\n",
    "            'f1': f1_score(all_targets, all_preds, average='macro', zero_division=0),\n",
    "            'bleu': sum(bleu_scores) / len(bleu_scores) if bleu_scores else 0.0,\n",
    "            'true_positive': tp,\n",
    "            'true_negative': tn.sum(),\n",
    "            'false_positive': fp.sum(),\n",
    "            'false_negative': fn.sum()\n",
    "        }\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" One head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" Multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\" A simple feed-forward network \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class GPTLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        logits = self.lm_head(x)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "# Load the model\n",
    "model_path = '/home/anon/training/model_trans.pkl' if os.path.exists('/home/anon/training/model_trans.pkl') else '/home/anon/training/model_sans.pkl'\n",
    "\n",
    "with open(model_path, 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "start_time = time.time()\n",
    "for iter in range(max_iters):\n",
    "    xb, yb, _ = get_batch('train', batch_size)\n",
    "    if xb is None or yb is None:\n",
    "        continue\n",
    "    xb = xb.view(-1, xb.size(-1))\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if iter % eval_iters == 0:\n",
    "        metrics = estimate_loss_and_metrics(model)\n",
    "        end_time = time.time()\n",
    "        send_notification(\n",
    "            \"Hindi Transfer Training\",\n",
    "            f\"step: {iter}, train loss: {metrics['train']['loss']:.3f}, val loss: {metrics['val']['loss']:.3f}, \"\n",
    "            f\"train accuracy: {metrics['train']['accuracy']:.3f}, val accuracy: {metrics['val']['accuracy']:.3f}, \"\n",
    "            f\"train precision: {metrics['train']['precision']:.3f}, val precision: {metrics['val']['precision']:.3f}, \"\n",
    "            f\"train recall: {metrics['train']['recall']:.3f}, val recall: {metrics['val']['recall']:.3f}, \"\n",
    "            f\"train f1: {metrics['train']['f1']:.3f}, val f1: {metrics['val']['f1']:.3f}, \"\n",
    "            f\"train BLEU: {metrics['train']['bleu']:.3f}, val BLEU: {metrics['val']['bleu']:.3f}, \"\n",
    "            f\"train TP: {metrics['train']['true_positive']}, val TP: {metrics['val']['true_positive']}, \"\n",
    "            f\"train TN: {metrics['train']['true_negative']}, val TN: {metrics['val']['true_negative']}, \"\n",
    "            f\"train FP: {metrics['train']['false_positive']}, val FP: {metrics['val']['false_positive']}, \"\n",
    "            f\"train FN: {metrics['train']['false_negative']}, val FN: {metrics['val']['false_negative']} \"\n",
    "            f\"in {(end_time - start_time):.2f} seconds.\"\n",
    "        )\n",
    "\n",
    "    if iter % eval_iters == 0 and iter != 0:\n",
    "        # Update trans_vocab\n",
    "        seen_characters = set()  # Define how seen_characters are gathered\n",
    "        characters_to_forget = trans_vocab - seen_characters\n",
    "        characters_to_learn = seen_characters - trans_vocab\n",
    "\n",
    "        if characters_to_forget and characters_to_learn:\n",
    "            \n",
    "            num_forget = max(1, int(forget_retension_percentage * len(trans_vocab)))  # Ensure at least 1 character is removed\n",
    "            num_learn = max(1, int(forget_retension_percentage * len(trans_vocab)))  # Ensure at least 1 character is added\n",
    "            learn_samples = random.sample(list(characters_to_learn), min(num_learn, len(characters_to_learn)))\n",
    "            forget_samples = random.sample(list(characters_to_forget), min(num_forget, len(characters_to_forget)))\n",
    "            if len(learn_samples) == len(forget_samples):\n",
    "                trans_vocab -= set(forget_samples)\n",
    "                trans_vocab.update(learn_samples)\n",
    "\n",
    "        print(vocab_size)\n",
    "\n",
    "print("Training complete.")"
    "# Save the model\n",
    "model_save_path = '/home/anon/training/model_trans.pkl'\n",
    "with open(model_save_path, 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "# Save trans_vocab at the end\n",
    "with open('/home/anon/training/trans_vocab.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(''.join(sorted(trans_vocab)))\n",
    "\n",
    "send_notification(\"Hindi Transfer Training\", \"Model training complete and saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
